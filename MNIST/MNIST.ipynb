{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a9c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad88cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "\n",
    "train_data = datasets.MNIST('./datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST('./datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f4b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_h(nn.Module):\n",
    "    def __init__(self, hidden_units=[512,256,128]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dim = 28 * 28    # MNIST\n",
    "        self.out_dim = 10\n",
    "        \n",
    "        self.l_layers = nn.ModuleList()    # nn.module을 저장하는 역할을 함, Module의 존재를 PyTorch에게 알려줌\n",
    "        self.l_layers.append(nn.Linear(self.in_dim, hidden_units[0]))    # 선형 변환 (입력 텐서 크기, 출력 텐서 크기)\n",
    "        for i in range(len(hidden_units)-1):\n",
    "            self.l_layers.append(nn.Linear(hidden_units[i], hidden_units[i+1]))\n",
    "        self.l_layers.append(nn.Linear(hidden_units[-1], self.out_dim))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.log_softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = x.view(-1, self.in_dim)\n",
    "        for l in range(len(self.l_layers)):\n",
    "            z = self.l_layers[l](a)  # l번쨰 층에 입력 a를 전달함\n",
    "            if l == len(self.l_layers) - 1:\n",
    "                logit = z\n",
    "            else:\n",
    "                a = self.relu(z)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895af3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(hidden_units):\n",
    "    print(f\"{len(hidden_units)} Layers\")\n",
    "    print(hidden_units)\n",
    "    \n",
    "    model = MLP_h(hidden_units)\n",
    "    criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류를 위한 CrossEntropyLoss\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)  # SGD 사용\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data  # inputs(입력데이터), labels(정답데이터)\n",
    "            \n",
    "            optimizer.zero_grad()  # optimizer의 Gradient 값을 초기화함\n",
    "            \n",
    "            outputs = model(inputs)  # 모델에 입력값을 전달하여 예측값을 얻음\n",
    "            loss = criterion(outputs, labels)  # 손실 계산 (예측값, 실제값 활용)\n",
    "            loss.backward()  # Backward propagation을 통해 gradient 계산\n",
    "            optimizer.step()   # 계산된 Gradient를 사용하여 모델 파라미터 업데이트\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if (i+1) % 2000 == 0:\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    print('Finished Training')\n",
    "    \n",
    "    n_predict = 0\n",
    "    n_correct = 0\n",
    "    \n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        n_predict += len(predicted)\n",
    "        n_correct += (labels == predicted).sum()\n",
    "        \n",
    "    print(f\"{n_correct}/{n_predict}\")\n",
    "    print(f\"Accuracy: {n_correct/n_predict:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fff38ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Layers\n",
      "[2048, 1024, 512, 256, 128]\n",
      "[1,  2000] loss: 2.279\n",
      "[1,  4000] loss: 0.991\n",
      "[2,  2000] loss: 0.321\n",
      "[2,  4000] loss: 0.232\n",
      "[3,  2000] loss: 0.146\n",
      "[3,  4000] loss: 0.128\n",
      "[4,  2000] loss: 0.094\n",
      "[4,  4000] loss: 0.084\n",
      "[5,  2000] loss: 0.062\n",
      "[5,  4000] loss: 0.061\n",
      "[6,  2000] loss: 0.039\n",
      "[6,  4000] loss: 0.044\n",
      "[7,  2000] loss: 0.031\n",
      "[7,  4000] loss: 0.030\n",
      "[8,  2000] loss: 0.020\n",
      "[8,  4000] loss: 0.024\n",
      "[9,  2000] loss: 0.016\n",
      "[9,  4000] loss: 0.018\n",
      "[10,  2000] loss: 0.015\n",
      "[10,  4000] loss: 0.014\n",
      "Finished Training\n",
      "9769/10000\n",
      "Accuracy: 0.977\n"
     ]
    }
   ],
   "source": [
    "test([2048,1024,512,256,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe1987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
